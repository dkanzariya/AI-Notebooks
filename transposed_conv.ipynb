{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "Copy of transposed-conv.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkanzariya/Computer-Vision/blob/main/transposed_conv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWh7HstcDQXU"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgrF1MkEDQXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2613ec3c-61a9-4b2b-e9e5-9ef5fac5857f"
      },
      "source": [
        "!pip install d2l==0.16.1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting d2l==0.16.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/2b/3515cd6f2898bf95306a5c58b065aeb045fdc25516f2b68b0f8409e320c3/d2l-0.16.1-py3-none-any.whl (76kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 17.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20kB 21.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 30kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 51kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 61kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 71kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from d2l==0.16.1) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from d2l==0.16.1) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from d2l==0.16.1) (1.19.5)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from d2l==0.16.1) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.16.1) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.16.1) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.16.1) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.16.1) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l==0.16.1) (2018.9)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.16.1) (5.6.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.16.1) (4.10.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.16.1) (5.3.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.16.1) (5.2.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.16.1) (7.6.3)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.16.1) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->d2l==0.16.1) (1.15.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.16.1) (0.4.4)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.16.1) (2.11.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.16.1) (2.6.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.16.1) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.16.1) (0.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.16.1) (4.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.16.1) (3.3.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.16.1) (1.4.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.16.1) (0.6.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.16.1) (4.3.3)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.16.1) (5.1.2)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l==0.16.1) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l==0.16.1) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l==0.16.1) (5.3.5)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.16.1) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.16.1) (0.9.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.16.1) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l==0.16.1) (1.0.18)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l==0.16.1) (1.0.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l==0.16.1) (3.5.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.16.1) (1.9.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.16.1) (22.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter->d2l==0.16.1) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.16.1) (20.9)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.16.1) (0.5.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->jupyter->d2l==0.16.1) (4.4.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert->jupyter->d2l==0.16.1) (2.6.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.1) (53.0.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.1) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.1) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.1) (0.8.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==0.16.1) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l==0.16.1) (0.2.5)\n",
            "Installing collected packages: d2l\n",
            "Successfully installed d2l-0.16.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "sizMpiyXDQXb"
      },
      "source": [
        "# Transposed Convolution\n",
        ":label:`sec_transposed_conv`\n",
        "\n",
        "The layers we introduced so far for convolutional neural networks, including\n",
        "convolutional layers (:numref:`sec_conv_layer`) and pooling layers (:numref:`sec_pooling`), often reduce the input width and height, or keep them unchanged. Applications such as semantic segmentation (:numref:`sec_semantic_segmentation`) and generative adversarial networks (:numref:`sec_dcgan`), however, require to predict values for each pixel and therefore needs to increase input width and height. Transposed convolution, also named fractionally-strided convolution :cite:`Dumoulin.Visin.2016` or deconvolution :cite:`Long.Shelhamer.Darrell.2015`, serves this purpose.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "pAc0FzLbDQXb"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 3,
        "id": "v_2Yj4wiDQXb"
      },
      "source": [
        "## Basic 2D Transposed Convolution\n",
        "\n",
        "Let us consider a basic case that both input and output channels are 1, with 0 padding and 1 stride. :numref:`fig_trans_conv` illustrates how transposed convolution with a $2\\times 2$ kernel is computed on the $2\\times 2$ input matrix.\n",
        "\n",
        "![Transposed convolution layer with a $2\\times 2$ kernel.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/trans-conv.svg?raw=1)\n",
        ":label:`fig_trans_conv`\n",
        "\n",
        "We can implement this operation by giving matrix kernel $K$ and matrix input $X$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 4,
        "tab": [
          "pytorch"
        ],
        "id": "-Q2Gf5FsDQXc"
      },
      "source": [
        "def trans_conv(X, K):\n",
        "    h, w = K.shape\n",
        "    Y = torch.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))\n",
        "    for i in range(X.shape[0]):\n",
        "        for j in range(X.shape[1]):\n",
        "            Y[i: i + h, j: j + w] += X[i, j] * K\n",
        "    return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 5,
        "id": "9IWgCt7QDQXc"
      },
      "source": [
        "Remember the convolution computes results by `Y[i, j] = (X[i: i + h, j: j + w] * K).sum()` (refer to `corr2d` in :numref:`sec_conv_layer`), which summarizes input values through the kernel. While the transposed convolution broadcasts input values through the kernel, which results in a larger output shape.\n",
        "\n",
        "Verify the results in :numref:`fig_trans_conv`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 6,
        "tab": [
          "pytorch"
        ],
        "id": "H6hrnW38DQXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf53fc6-87f4-442c-d545-566f21b0cd57"
      },
      "source": [
        "X = torch.tensor([[0., 1], [2, 3]])\n",
        "K = torch.tensor([[0., 1], [2, 3]])\n",
        "trans_conv(X, K)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  0.,  1.],\n",
              "        [ 0.,  4.,  6.],\n",
              "        [ 4., 12.,  9.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "WFoNwn9ADQXd"
      },
      "source": [
        "Or we can use `nn.ConvTranspose2d` to obtain the same results. As `nn.Conv2d`, both input and kernel should be 4-D tensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "id": "P8C17n4BDQXd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f3ab9c-0ee1-4047-b596-41c2676831ac"
      },
      "source": [
        "X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)\n",
        "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)\n",
        "tconv.weight.data = K\n",
        "tconv(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.,  0.,  1.],\n",
              "          [ 0.,  4.,  6.],\n",
              "          [ 4., 12.,  9.]]]], grad_fn=<SlowConvTranspose2DBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 11,
        "id": "ypyxItqtDQXe"
      },
      "source": [
        "## Padding, Strides, and Channels\n",
        "\n",
        "We apply padding elements to the input in convolution, while they are applied to the output in transposed convolution. A $1\\times 1$ padding means we first compute the output as normal, then remove the first/last rows and columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "WaBw0GKNDQXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae0f955f-497c-4eb2-8b73-fbc32b29a565"
      },
      "source": [
        "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)\n",
        "tconv.weight.data = K\n",
        "tconv(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[4.]]]], grad_fn=<SlowConvTranspose2DBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 14,
        "id": "nqLMpd0bDQXe"
      },
      "source": [
        "Similarly, strides are applied to outputs as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "hYWRcbwZDQXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3c1dad-ac37-482f-d180-c57782e6413d"
      },
      "source": [
        "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)\n",
        "tconv.weight.data = K\n",
        "tconv(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 0., 0., 1.],\n",
              "          [0., 0., 2., 3.],\n",
              "          [0., 2., 0., 3.],\n",
              "          [4., 6., 6., 9.]]]], grad_fn=<SlowConvTranspose2DBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 17,
        "id": "gjxrokxlDQXf"
      },
      "source": [
        "The multi-channel extension of the transposed convolution is the same as the convolution. When the input has multiple channels, denoted by $c_i$, the transposed convolution assigns a $k_h\\times k_w$ kernel matrix to each input channel. If the output has a channel size $c_o$, then we have a $c_i\\times k_h\\times k_w$ kernel for each output channel.\n",
        "\n",
        "\n",
        "As a result, if we feed $X$ into a convolutional layer $f$ to compute $Y=f(X)$ and create a transposed convolution layer $g$ with the same hyperparameters as $f$ except for the output channel set to be the channel size of $X$, then $g(Y)$ should has the same shape as $X$. Let us verify this statement.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 19,
        "tab": [
          "pytorch"
        ],
        "id": "uegMwy2_DQXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b6e1747-3615-49b9-fe95-9d1c042d45b3"
      },
      "source": [
        "X = torch.rand(size=(1, 10, 16, 16))\n",
        "conv = nn.Conv2d(10, 20, kernel_size=5, padding=2, stride=3)\n",
        "tconv = nn.ConvTranspose2d(20, 10, kernel_size=5, padding=2, stride=3)\n",
        "tconv(conv(X)).shape == X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 20,
        "id": "hz2AbrfTDQXg"
      },
      "source": [
        "## Analogy to Matrix Transposition\n",
        "\n",
        "The transposed convolution takes its name from the matrix transposition. In fact, convolution operations can also be achieved by matrix multiplication. In the example below, we define a $3\\times 3$ input $X$ with a $2\\times 2$ kernel $K$, and then use `corr2d` to compute the convolution output.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 21,
        "tab": [
          "pytorch"
        ],
        "id": "Imbpmr4ZDQXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c80fcf-09a8-4217-d2c2-b4c8fb8b22b4"
      },
      "source": [
        "X = torch.arange(9.0).reshape(3, 3)\n",
        "K = torch.tensor([[0, 1], [2, 3]])\n",
        "Y = d2l.corr2d(X, K)\n",
        "Y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[19., 25.],\n",
              "        [37., 43.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 22,
        "id": "5lH0UwizDQXg"
      },
      "source": [
        "Next, we rewrite convolution kernel $K$ as a matrix $W$. Its shape will be $(4, 9)$, where the $i^\\mathrm{th}$ row present applying the kernel to the input to generate the $i^\\mathrm{th}$ output element.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 23,
        "tab": [
          "pytorch"
        ],
        "id": "E5IojPPRDQXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5351bca7-c6b4-4e19-86e4-eca26cc9b04d"
      },
      "source": [
        "def kernel2matrix(K):\n",
        "    k, W = torch.zeros(5), torch.zeros((4, 9))\n",
        "    k[:2], k[3:5] = K[0, :], K[1, :]\n",
        "    W[0, :5], W[1, 1:6], W[2, 3:8], W[3, 4:] = k, k, k, k\n",
        "    return W\n",
        "\n",
        "W = kernel2matrix(K)\n",
        "W"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1., 0., 2., 3., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 2., 3., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 2., 3., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 2., 3.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 24,
        "id": "mtBdzE1zDQXh"
      },
      "source": [
        "Then the convolution operator can be implemented by matrix multiplication with proper reshaping.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 26,
        "tab": [
          "pytorch"
        ],
        "id": "UXHZtDAvDQXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b26ff39-53cb-4078-fd56-0d6eb7f01cd1"
      },
      "source": [
        "Y == torch.mv(W, X.reshape(-1)).reshape(2, 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True],\n",
              "        [True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 27,
        "id": "Sc9oyyKSDQXh"
      },
      "source": [
        "We can implement transposed convolution as a matrix multiplication as well by reusing `kernel2matrix`. To reuse the generated $W$, we construct a $2\\times 2$ input, so the corresponding weight matrix will have a shape $(9, 4)$, which is $W^\\top$. Let us verify the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 29,
        "tab": [
          "pytorch"
        ],
        "id": "99mMg3JvDQXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a242cb1-4926-4cb6-d54f-06dd000c4f9b"
      },
      "source": [
        "X = torch.tensor([[0.0, 1], [2, 3]])\n",
        "Y = trans_conv(X, K)\n",
        "Y == torch.mv(W.T, X.reshape(-1)).reshape(3, 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True],\n",
              "        [True, True, True],\n",
              "        [True, True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 30,
        "id": "kex4ZUXJDQXi"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* Compared to convolutions that reduce inputs through kernels, transposed convolutions broadcast inputs.\n",
        "* If a convolution layer reduces the input width and height by $n_w$ and $h_h$ time, respectively. Then a transposed convolution layer with the same kernel sizes, padding and strides will increase the input width and height by $n_w$ and $n_h$, respectively.\n",
        "* We can implement convolution operations by the matrix multiplication, the corresponding transposed convolutions can be done by transposed matrix multiplication.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Is it efficient to use matrix multiplication to implement convolution operations? Why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 32,
        "tab": [
          "pytorch"
        ],
        "id": "PJEg6EFjDQXi"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/1450)\n"
      ]
    }
  ]
}